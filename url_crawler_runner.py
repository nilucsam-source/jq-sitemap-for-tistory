# url_crawler_runner.py
# [ìš´ì˜ìš©] ì‹¤ì œ ë¸”ë¡œê·¸ í˜ì´ì§€ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ sitemap ìƒì„±

import logging

import requests
from bs4 import BeautifulSoup

from core.find_best_selector import find_best_selector
from core.generate_sitemap import generate_sitemap_from_soup
from core.utils import load_config, save_config, write_sitemap

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger(__name__)

CONFIG_PATH = "config.json"

def main():
    config = load_config(CONFIG_PATH)
    blog_url = config["blog_url"].rstrip("/")
    max_pages = config.get("max_pages", 5)
    selector_config = config["selector"]
    output_file = config.get("output_file", "sitemap.xml")

    all_post_infos = []
    seen_urls = set()

    for page in range(1, max_pages + 1):
        url = f"{blog_url}/category?page={page}"
        logger.info(f"ğŸŒ í¬ë¡¤ë§: {url}")
        res = requests.get(url)
        if res.status_code != 200:
            logger.error(f"âŒ í˜ì´ì§€ ì‹¤íŒ¨: {url}")
            continue

        soup = BeautifulSoup(res.text, "html.parser")    
        main = soup.find("main") or soup

        # is_initialized: ìë™ ì…€ë ‰í„° íƒì§€ ì‹¤í–‰
        if not config.get("is_initialized", False):
            logger.info("ğŸ” is_initialized: False â†’ ì…€ë ‰í„° íƒì§€ ê°ì§€ ì¤‘...")
            detected = find_best_selector(main, selector_config)
            config["selector"].update(detected)
            config["is_initialized"] = True
            save_config(config, CONFIG_PATH)
            logger.info("âœ… ì…€ë ‰í„° ì €ì¥ ì™„ë£Œ!")

        # sitemap ì •ë³´ ìˆ˜ì§‘
        post_infos = generate_sitemap_from_soup(blog_url, soup, selector_config)
        # all_post_infos.extend(post_infos)
        
        for info in post_infos:
            url = info["url"]
            if url in seen_urls:
                logger.warning(f"âš ï¸ ì „ì²´ ì¤‘ë³µ URL ìŠ¤í‚µ: {url}")
                continue
            seen_urls.add(url)
            all_post_infos.append(info)

    # sitemap ìƒì„±
    write_sitemap(all_post_infos, output_file)
    logger.info(f"âœ… sitemap.xml ì €ì¥ ì™„ë£Œ â†’ {output_file}")

if __name__ == "__main__":
    logger.info("âœ… ìš´ì˜ ëª¨ë“œ ì‹¤í–‰ì¤‘")
    main()